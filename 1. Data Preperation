{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6799366,"sourceType":"datasetVersion","datasetId":3911893},{"sourceId":6799390,"sourceType":"datasetVersion","datasetId":3911912},{"sourceId":6799414,"sourceType":"datasetVersion","datasetId":3911927},{"sourceId":6799433,"sourceType":"datasetVersion","datasetId":3911941},{"sourceId":6799540,"sourceType":"datasetVersion","datasetId":3912006},{"sourceId":6799570,"sourceType":"datasetVersion","datasetId":3912027},{"sourceId":6799597,"sourceType":"datasetVersion","datasetId":3912048},{"sourceId":6799776,"sourceType":"datasetVersion","datasetId":3912165},{"sourceId":6799875,"sourceType":"datasetVersion","datasetId":3912239},{"sourceId":6813045,"sourceType":"datasetVersion","datasetId":3919261},{"sourceId":6813221,"sourceType":"datasetVersion","datasetId":3919319},{"sourceId":6813317,"sourceType":"datasetVersion","datasetId":3919346},{"sourceId":6874789,"sourceType":"datasetVersion","datasetId":3950453},{"sourceId":6874900,"sourceType":"datasetVersion","datasetId":3950509}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  **Marine Route Optimization Using Machine Learning**","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Collection and Preprocessing","metadata":{}},{"cell_type":"code","source":"# Imports \n\nimport requests # for making HTTP requests\n\nimport zipfile # fro working with zip files\n\nimport os # for interacting with OS\n\nimport pandas as pd\n\nimport ftplib # for FTP operations\n\nfrom getpass import getpass #to securely input a password\n\nimport xarray as xr # for working with multi-dimentional arrays and datasets\n\nimport numpy as np\n\nfrom datetime import datetime, timedelta \n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:51.710228Z","iopub.execute_input":"2023-11-22T07:24:51.710694Z","iopub.status.idle":"2023-11-22T07:24:52.053854Z","shell.execute_reply.started":"2023-11-22T07:24:51.710662Z","shell.execute_reply":"2023-11-22T07:24:52.052642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"STEP 1: Collect data\n\nAIS and CMEMS data for the model on:\n\n* 01.01.2023\n* 01.04.2023\n* 01.07.2023\n* 01.10.2023\n\nCMEMS data for the routing on:\n\n* 01.06.2023\n* 02.06.2023\n* 03.06.2023\n\n# **Model data- AIS**","metadata":{}},{"cell_type":"code","source":"\n\n\"\"\"# Download data\n\n# Set file path\nurls = ['https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2023/AIS_2023_01_01.zip',\n'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2023/AIS_2023_04_01.zip',\n#'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2020/AIS_2020_07_01.zip',\n#'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2020/AIS_2020_10_01.zip'\n       ]\n\n# Unzip data\nfor url in urls:\n\n  r = requests.get(url)\n  filename = url.split('/')[-1]\n  with open(filename,'wb') as output_file:  # open new file with same name in binary write mode.\n      output_file.write(r.content)          # writes the binary content of hTTP response\n  print(filename)\n\n  try:\n      with zipfile.ZipFile(filename) as z:\n          z.extractall()\n          print(\"Extracted file\")\n          os.remove(filename)\n  except:\n      print(\"Invalid file\")\n\nprint(\"Download completed!\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.055658Z","iopub.execute_input":"2023-11-22T07:24:52.056231Z","iopub.status.idle":"2023-11-22T07:24:52.217780Z","shell.execute_reply.started":"2023-11-22T07:24:52.056197Z","shell.execute_reply":"2023-11-22T07:24:52.216678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we define a list called 'urls' that contain 4 URLs pointing to ZIP files. These ZIP files contain data related to AIS (Automatic Identification System.)\n\nNext, we beging a loop to process each URL in the 'urls' list. \nInside the loop, it sends an **HTTP GET** request to the URL specified by url using the **requests.get method**. \nIt then extracts the last part of the URL (the filename) by splitting the URL using / and selecting the last element.\n\nNow, we are creating a new binary file with the same filename and writing the content of an HTTP response ('**r.content**') into that file. \n\nNext, we open the downloaded file as a ZIP archive using '**zipfile.ZipFile**'. If successful, it extrracts all the contents of the file to the current directoey using '**z.extractall()**'. If the extraction is succesful, it prints \"Extracted file\", and then the original ZIP file is removed using '**os.remove**'. If there's an issue with the opening the ZIP file, it prints \"Invalid file\"","metadata":{}},{"cell_type":"code","source":"# Read csv and create df\n\"\"\"\nais_data = pd.DataFrame()\n\nfor url in urls:\n    \n    filename = url.split('/')[-1]\n    filename = filename.replace(\"zip\", \"csv\")\n    filepath = '/kaggle/working/' + filename  \n\n    data = pd.read_csv(filepath)\n    ais_data = pd.concat([ais_data, data], ignore_index=True)  # Concatenate DataFrames.\n\nprint(\"Before preprocessing...\")\nais_data\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.221523Z","iopub.execute_input":"2023-11-22T07:24:52.221887Z","iopub.status.idle":"2023-11-22T07:24:52.233440Z","shell.execute_reply.started":"2023-11-22T07:24:52.221846Z","shell.execute_reply":"2023-11-22T07:24:52.232284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The goal here is to read CSV file and concatenate them into the 'ais_data' DataFrame.\n\nWe convert here the ZIP extension to CSV.\nThen we read the CSV file located at the 'filepath' and the data is loaded into a new pandas DataFrame 'data'.\n\nNext,the '**data**' DataFrame is concatenated with the '**ais_data**' DataFrame. The '**ignore_index=True**' argument ensures that the resulting DataFrame has a continuous index, as opposed to retaining the original indices from the individual DataFrames.","metadata":{}},{"cell_type":"code","source":"# Save AIS data to file\n\n\"\"\"ais_data.to_csv('/kaggle/working/ais_data_all.csv')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.236848Z","iopub.execute_input":"2023-11-22T07:24:52.237321Z","iopub.status.idle":"2023-11-22T07:24:52.248581Z","shell.execute_reply.started":"2023-11-22T07:24:52.237279Z","shell.execute_reply":"2023-11-22T07:24:52.247399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"ais_data_all= pd.read_csv('/kaggle/input/ais-all-data-jan01-apr04-2023/ais_data_all.csv')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.249969Z","iopub.execute_input":"2023-11-22T07:24:52.250380Z","iopub.status.idle":"2023-11-22T07:24:52.265144Z","shell.execute_reply.started":"2023-11-22T07:24:52.250352Z","shell.execute_reply":"2023-11-22T07:24:52.263976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"ais_data_all\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.267183Z","iopub.execute_input":"2023-11-22T07:24:52.268208Z","iopub.status.idle":"2023-11-22T07:24:52.281395Z","shell.execute_reply.started":"2023-11-22T07:24:52.268167Z","shell.execute_reply":"2023-11-22T07:24:52.280016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model data - CMEMS","metadata":{}},{"cell_type":"markdown","source":"Downloading the data for the 2 days.","metadata":{}},{"cell_type":"code","source":"# Connect to CMEMS FTP\n\"\"\"\nname = 'ksingh1'\npwd = 'Kunal@123'\n\nname = getpass('Enter name:')\npwd = getpass('Enter pwd: ')\n\n\ndef make_con(url):\n\n    con = ftplib.FTP(url)\n    print(con.getwelcome())\n\n    return con\n\ncon = make_con('nrt.cmems-du.eu')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.283156Z","iopub.execute_input":"2023-11-22T07:24:52.284029Z","iopub.status.idle":"2023-11-22T07:24:52.295732Z","shell.execute_reply.started":"2023-11-22T07:24:52.283989Z","shell.execute_reply":"2023-11-22T07:24:52.294260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"\nds1 = xr.open_dataset('/kaggle/input/jan2023-phy-ocean/cmems_mod_glo_phy_anfc_salinity0.083deg_PT1H-m_1698425838731.nc')\nds2 = xr.open_dataset('/kaggle/input/jan2023-phy-ocean/cmems_mod_glo_phy_anfc_theato0.083deg_PT1H-m_1698426299513.nc')\nds3 = xr.open_dataset('/kaggle/input/jan2023-phy-ocean/cmems_mod_glo_phy_anfc_thickness_0.083deg_P1D-m_1698437256627.nc')\n\n\n# Concatenation (if datasets have the same variables and dimensions)\nphy_jan = xr.merge([ds1, ds2, ds3])\n\nphy_jan.to_netcdf('jan_phy.nc')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.297650Z","iopub.execute_input":"2023-11-22T07:24:52.298229Z","iopub.status.idle":"2023-11-22T07:24:52.312309Z","shell.execute_reply.started":"2023-11-22T07:24:52.298190Z","shell.execute_reply":"2023-11-22T07:24:52.311359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phy_jan = xr.open_dataset('/kaggle/input/final-phy-jan-2023/jan_phy.nc')\nphy_jan","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:52.314028Z","iopub.execute_input":"2023-11-22T07:24:52.314749Z","iopub.status.idle":"2023-11-22T07:24:53.256435Z","shell.execute_reply.started":"2023-11-22T07:24:52.314709Z","shell.execute_reply":"2023-11-22T07:24:53.255544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wav_jan = xr.open_dataset('/kaggle/input/global-wav-ocean-analysis-202301jan/cmems_mod_glo_wav_anfc_0.083deg_PT3H-i_1698305046266.nc')\nwav_jan","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:53.261864Z","iopub.execute_input":"2023-11-22T07:24:53.262834Z","iopub.status.idle":"2023-11-22T07:24:53.384668Z","shell.execute_reply.started":"2023-11-22T07:24:53.262784Z","shell.execute_reply":"2023-11-22T07:24:53.383759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downlaoding the data for 01 APRIL 2023\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Physics Data**","metadata":{}},{"cell_type":"code","source":"#April jana combination\n\"\"\"\nds1 = xr.open_dataset('/kaggle/input/apr-phy2023/cmems_mod_glo_phy_anfc_salanity_0.083deg_PT1H-m_1698436627345.nc')\nds2 = xr.open_dataset('/kaggle/input/apr-phy2023/cmems_mod_glo_phy_anfc_thetao_0.083deg_PT1H-m_1698437097509.nc')\nds3 = xr.open_dataset('/kaggle/input/apr-phy2023/cmems_mod_glo_phy_anfc_thickness_0.083deg_P1D-m_1698442161874.nc')\n\n\n# Concatenation (if datasets have the same variables and dimensions)\nphy_apr = xr.merge([ds1, ds2, ds3])\n\nphy_apr.to_netcdf('apr_phy.nc')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:53.385982Z","iopub.execute_input":"2023-11-22T07:24:53.387047Z","iopub.status.idle":"2023-11-22T07:24:53.394103Z","shell.execute_reply.started":"2023-11-22T07:24:53.387016Z","shell.execute_reply":"2023-11-22T07:24:53.393046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phy_apr = xr.open_dataset('/kaggle/input/final-phy-data-2023/apr_phy.nc')\nphy_apr","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:53.396114Z","iopub.execute_input":"2023-11-22T07:24:53.396790Z","iopub.status.idle":"2023-11-22T07:24:53.548569Z","shell.execute_reply.started":"2023-11-22T07:24:53.396749Z","shell.execute_reply":"2023-11-22T07:24:53.547365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **Wave Data**","metadata":{}},{"cell_type":"code","source":"wav_apr = xr.open_dataset('/kaggle/input/global-wav-ocean-analysis-202301apr/cmems_mod_glo_wav_anfc_0.083deg_PT3H-i_1698305442484.nc')\nwav_apr\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:53.549752Z","iopub.execute_input":"2023-11-22T07:24:53.550547Z","iopub.status.idle":"2023-11-22T07:24:53.675697Z","shell.execute_reply.started":"2023-11-22T07:24:53.550516Z","shell.execute_reply":"2023-11-22T07:24:53.674339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **Selecting ship route area**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\n# Define the bounding box coordinates\nbbox = ((-65, 20), (-100, 43))\n\n# Create a Matplotlib figure and axis with a specific Cartopy projection\nfig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n\n# Add map features such as coastlines and borders\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS)\n\n# Set the map extent based on the bounding box\nax.set_extent([bbox[0][0], bbox[1][0], bbox[0][1], bbox[1][1]])\n\n# Add text labels for country names and port names\nlabel_properties = {\n    'fontweight': 'normal',\n    'fontsize': 10,\n    'ha': 'center',\n    'va': 'center',\n}\n\n# Define city and country labels with coordinates\nlabels = {\n    'Washington': (-73.69, 40.82, 'USA', 'red'),\n    \n    'Havana': (-82.3, 23.1, 'Mexico', 'red'),\n}\n\nfor city, (lon, lat, country, color) in labels.items():\n    ax.text(lon, lat, city, color=color, **label_properties)\n    ax.text(lon, lat - 1, country, color=color, **label_properties)\n\n# Differentiate between land and ocean by setting colors\nland = cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='face', facecolor='darkgrey')\nocean = cfeature.NaturalEarthFeature('physical', 'ocean', '50m', edgecolor='face', facecolor='lightblue')\n\nax.add_feature(land, zorder=0)\nax.add_feature(ocean, zorder=0)\n\n# Add a title\nplt.title(\"Shipping route area between port Dubai and Mumbai\")\n\n# Show the map\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:24:53.677053Z","iopub.execute_input":"2023-11-22T07:24:53.677521Z","iopub.status.idle":"2023-11-22T07:25:00.339518Z","shell.execute_reply.started":"2023-11-22T07:24:53.677485Z","shell.execute_reply":"2023-11-22T07:25:00.338636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Reduce CMEMS wave data size by using bbox**","metadata":{}},{"cell_type":"code","source":"\n#bbox = ((-60, 20), (-120, 50))\n\n\ndef get_closest(array, value):\n    return np.abs(array - value).argmin()\n    \nds_wav_jan = wav_jan\nds_wav_apr = wav_apr\n\n\nds_wav_all = [ds_wav_jan, ds_wav_apr]\n\ndatasets_wav = []\n\nfor ds_month in ds_wav_all:\n  \n  lon_min = get_closest(ds_month.longitude.data, bbox[0][0])\n  lon_max = get_closest(ds_month.longitude.data, bbox[1][0])\n  lat_min = get_closest(ds_month.latitude.data, bbox[0][1])\n  lat_max = get_closest(ds_month.latitude.data, bbox[1][1])\n\n  ds_wav_month_reg = ds_month.isel(time = 0, longitude = slice(lon_min, lon_max), latitude = slice(lat_min, lat_max))\n  datasets_wav.append(ds_wav_month_reg)\n\nds_wav = xr.concat(datasets_wav, dim = 'time')\nds_wav\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:25:00.340718Z","iopub.execute_input":"2023-11-22T07:25:00.341817Z","iopub.status.idle":"2023-11-22T07:25:00.386044Z","shell.execute_reply.started":"2023-11-22T07:25:00.341787Z","shell.execute_reply":"2023-11-22T07:25:00.384974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Reduce CMEMS physics data size by using bbox**","metadata":{}},{"cell_type":"code","source":"#bbox = ((-60, 20), (-120, 50))\n\n\nds_phy_jan =  phy_jan\nds_phy_apr =  phy_apr\n\n\nds_phy_all = [ds_phy_jan, ds_phy_apr]\n\ndatasets_phy = []\n\nfor ds_month in ds_phy_all:\n  \n  lon_min = get_closest(ds_month.longitude.data, bbox[0][0])\n  lon_max = get_closest(ds_month.longitude.data, bbox[1][0])\n  lat_min = get_closest(ds_month.latitude.data, bbox[0][1])\n  lat_max = get_closest(ds_month.latitude.data, bbox[1][1])\n\n  ds_phy_month_reg = ds_month.isel(time = 0, longitude = slice(lon_min, lon_max), latitude = slice(lat_min, lat_max))\n  datasets_phy.append(ds_phy_month_reg)\n\nds_phy = xr.concat(datasets_phy, dim = 'time')\nds_phy\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:25:00.387672Z","iopub.execute_input":"2023-11-22T07:25:00.387992Z","iopub.status.idle":"2023-11-22T07:25:00.426602Z","shell.execute_reply.started":"2023-11-22T07:25:00.387965Z","shell.execute_reply":"2023-11-22T07:25:00.425535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join all wave products by using open_mfdataset, chunking data in response to memory issues\nds_wav_all = xr.open_mfdataset('/kaggle/input/combined-wav-sliced-jan-apr-2023/cmems_mod_glo_wav_anfc*.nc')\n\nds_wav_all","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:25:00.427841Z","iopub.execute_input":"2023-11-22T07:25:00.428175Z","iopub.status.idle":"2023-11-22T07:25:00.737871Z","shell.execute_reply.started":"2023-11-22T07:25:00.428148Z","shell.execute_reply":"2023-11-22T07:25:00.734927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_phy_all = xr.open_mfdataset('/kaggle/input/new-combined-phy-jan-apr-2023/cmems_mod_glo_phy_anfc_*.nc')\nds_phy_all","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:25:00.740695Z","iopub.execute_input":"2023-11-22T07:25:00.741851Z","iopub.status.idle":"2023-11-22T07:25:01.470878Z","shell.execute_reply.started":"2023-11-22T07:25:00.741807Z","shell.execute_reply":"2023-11-22T07:25:01.469789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Routing Data -  CMEMS**","metadata":{}},{"cell_type":"code","source":"\nphy_jun = xr.open_dataset('/kaggle/input/routing-ocean-physics-202301-june/cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i_1698267716666.nc')\nphy_jun","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:25:01.472354Z","iopub.execute_input":"2023-11-22T07:25:01.473449Z","iopub.status.idle":"2023-11-22T07:25:01.556160Z","shell.execute_reply.started":"2023-11-22T07:25:01.473406Z","shell.execute_reply":"2023-11-22T07:25:01.554981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwav_jun = xr.open_dataset('/kaggle/input/routing-ocean-wav-202301-june/cmems_mod_glo_wav_anfc_0.083deg_PT3H-i_1698308781382.nc')\nwav_jun","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:25:01.557857Z","iopub.execute_input":"2023-11-22T07:25:01.558325Z","iopub.status.idle":"2023-11-22T07:25:01.652051Z","shell.execute_reply.started":"2023-11-22T07:25:01.558284Z","shell.execute_reply":"2023-11-22T07:25:01.651064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **STEP 2: Merge and Preprocess Model Data**","metadata":{}},{"cell_type":"markdown","source":"\nIn general:\n\n* Merge AIS data with CMEMS data\n* Remove unneccessary columns (show estimated time, latitude, longitude, heading, SOG, COG, Gross Tonage, VHM0, VMDR, VTPK, Temperature, Salinity and Thickness)\n\nFor all data:\n\n* Remove ships with Status =! 0 or Status =! 8\n* Remove ships with SOG < 7 or SOG > 102.2\n* Remove ships with latitude > 91 and longitude > 181\n* Remove ships with latitude < -91 and longitude < -181\n* Remove ships with heading > 361\n* Round data base times\n* Normalize all data to remove outliers\n* Standardize between 0 and 1","metadata":{}},{"cell_type":"code","source":"study_data = pd.read_csv('/kaggle/input/ais-all-data-jan01-apr04-2023/ais_data_all.csv')\nstudy_data","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:25:01.653511Z","iopub.execute_input":"2023-11-22T07:25:01.653965Z","iopub.status.idle":"2023-11-22T07:26:22.450884Z","shell.execute_reply.started":"2023-11-22T07:25:01.653929Z","shell.execute_reply":"2023-11-22T07:26:22.449776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Scatter plot of 'LAT' and 'LON'\nplt.scatter(study_data['LON'], study_data['LAT'], s=1)  # Adjust 's' for point size\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Spatial Distribution of AIS Data')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:22.452332Z","iopub.execute_input":"2023-11-22T07:26:22.452738Z","iopub.status.idle":"2023-11-22T07:26:28.211847Z","shell.execute_reply.started":"2023-11-22T07:26:22.452698Z","shell.execute_reply":"2023-11-22T07:26:28.210558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove ships with status =! 0 and status =! 8\nstudy_data = study_data[(study_data['Status'] == 0) | (study_data['Status'] == 8)].dropna()\n\n# Remove ships with SOG < 5 or SOG > 102.2\nstudy_data = study_data[(study_data['SOG'] > 7) & (study_data['SOG'] < 102.2)].dropna()\n\n# Remove ships with latitude > 91 and longitude > 181\nstudy_data = study_data[(study_data['LAT'] >= 20) & (study_data['LAT'] <= 43)].dropna()\nstudy_data = study_data[(study_data['LON'] >= -87.5) & (study_data['LON'] <= -72.5)].dropna()\n\n# Remove ships with heading > 361\nstudy_data = study_data[(study_data['Heading'] < 361)].dropna()\n\n# Calculate tonnage (Length * Breadth * Depth * S) - WE DON'T HAVE THE DEPTH\n# According to https://cdn.shopify.com/s/files/1/1021/8837/files/Tonnage_Guide_1_-_Simplified_Measurement.pdf?1513\nstudy_data['GrossTonnage'] = 0.67 * study_data['Length'] * study_data['Width']\n\nstudy_data = study_data.reset_index(drop=True)\nstudy_data\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:28.213953Z","iopub.execute_input":"2023-11-22T07:26:28.214331Z","iopub.status.idle":"2023-11-22T07:26:45.644318Z","shell.execute_reply.started":"2023-11-22T07:26:28.214290Z","shell.execute_reply":"2023-11-22T07:26:45.643004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Scatter plot of 'LAT' and 'LON'\nplt.scatter(study_data['LON'], study_data['LAT'], s=1)  # Adjust 's' for point size\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Spatial Distribution of AIS Data')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:45.645584Z","iopub.execute_input":"2023-11-22T07:26:45.645933Z","iopub.status.idle":"2023-11-22T07:26:46.013864Z","shell.execute_reply.started":"2023-11-22T07:26:45.645905Z","shell.execute_reply":"2023-11-22T07:26:46.012689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we round the data base times.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, timedelta\n\ndef datetime_rounder(time_str):\n    time = datetime.fromisoformat(time_str)\n    rounded_time = time.replace(second=0, minute=0, hour=time.hour) + timedelta(minutes=time.minute // 30)\n    return rounded_time.strftime('%y-%m-%d %H:%M:%S')\n\nstudy_data['EstimatedTime'] = study_data['BaseDateTime'].apply(datetime_rounder)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:46.015059Z","iopub.execute_input":"2023-11-22T07:26:46.015401Z","iopub.status.idle":"2023-11-22T07:26:49.222869Z","shell.execute_reply.started":"2023-11-22T07:26:46.015373Z","shell.execute_reply":"2023-11-22T07:26:49.221660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_data","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:49.224298Z","iopub.execute_input":"2023-11-22T07:26:49.224638Z","iopub.status.idle":"2023-11-22T07:26:49.267797Z","shell.execute_reply.started":"2023-11-22T07:26:49.224609Z","shell.execute_reply":"2023-11-22T07:26:49.266753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standardization between 0 and 1","metadata":{}},{"cell_type":"code","source":"\n# Get data for one time\nds_wav = ds_wav_all\nds_phy = ds_phy_all\nlat = round(study_data['LAT'].iloc[0])\nlon = round(study_data['LON'].iloc[0])\ntime = str(study_data['EstimatedTime'].iloc[0])\n\nVHM0 = ds_wav.VHM0.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\nVMDR = ds_wav.VMDR.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\nthetao = ds_phy.thetao.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')\nsalin = ds_phy.so.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:49.268716Z","iopub.execute_input":"2023-11-22T07:26:49.269036Z","iopub.status.idle":"2023-11-22T07:26:49.300479Z","shell.execute_reply.started":"2023-11-22T07:26:49.269009Z","shell.execute_reply":"2023-11-22T07:26:49.299313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_data","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:49.306674Z","iopub.execute_input":"2023-11-22T07:26:49.307036Z","iopub.status.idle":"2023-11-22T07:26:49.348956Z","shell.execute_reply.started":"2023-11-22T07:26:49.307006Z","shell.execute_reply":"2023-11-22T07:26:49.347791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:49.351162Z","iopub.execute_input":"2023-11-22T07:26:49.351917Z","iopub.status.idle":"2023-11-22T07:26:49.356452Z","shell.execute_reply.started":"2023-11-22T07:26:49.351885Z","shell.execute_reply":"2023-11-22T07:26:49.355351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DATA SAMPLING\n\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Perform data sampling (adjust the fraction as needed)\nsampled_data = study_data.sample(frac=0.1, random_state=42)  # 10% of the data\n\n# Create new columns with initial values\nsampled_data['VHM0'] = 0.0\nsampled_data['VMDR'] = 0.0\nsampled_data['VTPK'] = 0.0\nsampled_data['Temperature'] = 0.0\nsampled_data['Salinity'] = 0.0\nsampled_data['Thickness'] = 0.0\n\ndef extract_model():\n    # Use tqdm to create a progress bar\n    for index, row in tqdm(sampled_data.iterrows(), total=len(sampled_data), desc=\"Processing\"):\n        lat = round(row['LAT'], 1)\n        lon = round(row['LON'], 1)\n        time = str(row['EstimatedTime'])\n\n        VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n        sampled_data.at[index, 'VHM0'] = VHM0\n        sampled_data.at[index, 'VMDR'] = VMDR\n        sampled_data.at[index, 'VTPK'] = VTPK\n        sampled_data.at[index, 'Temperature'] = thetao\n        sampled_data.at[index, 'Salinity'] = salin\n        sampled_data.at[index, 'Thickness'] = thickness\n\n# Call the function to extract the model\nextract_model()\n\n# Drop rows with missing values\nsampled_data = sampled_data.dropna()\n\n# Display the processed data\nsampled_data\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:26:49.357744Z","iopub.execute_input":"2023-11-22T07:26:49.358084Z","iopub.status.idle":"2023-11-22T07:27:14.552545Z","shell.execute_reply.started":"2023-11-22T07:26:49.358056Z","shell.execute_reply":"2023-11-22T07:27:14.551181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.554188Z","iopub.status.idle":"2023-11-22T07:27:14.554850Z","shell.execute_reply.started":"2023-11-22T07:27:14.554646Z","shell.execute_reply":"2023-11-22T07:27:14.554667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utilizing 12 cores below \n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Create new columns with initial values\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\nstudy_data['Temperature'] = 0.0\nstudy_data['Salinity'] = 0.0\nstudy_data['Thickness'] = 0.0\n\ndef extract_model(args):\n    index, row = args\n    lat = round(row['LAT'], 1)\n    lon = round(row['LON'], 1)\n    time = str(row['EstimatedTime'])\n\n    VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n    VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n    VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n    thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n    salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n    thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n    study_data.at[index, 'VHM0'] = VHM0\n    study_data.at[index, 'VMDR'] = VMDR\n    study_data.at[index, 'VTPK'] = VTPK\n    study_data.at[index, 'Temperature'] = thetao\n    study_data.at[index, 'Salinity'] = salin\n    study_data.at[index, 'Thickness'] = thickness\n\n# Use ThreadPoolExecutor to parallelize the processing\nwith ThreadPoolExecutor(max_workers=12) as executor:\n    list(tqdm(executor.map(extract_model, study_data.iterrows()), total=len(study_data), desc=\"Processing\"))\n\n# Drop rows with missing values\nstudy_data = study_data.dropna()\n\n\n# Save merge data (not fully processed)\nstudy_data.to_csv('data/study_data_np.csv')\n\n# Display the processed data\nstudy_data","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:29:38.135133Z","iopub.execute_input":"2023-11-22T07:29:38.135575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The goal is to merge AIS (Automatic Identification System) and CMEMS (Copernicus Marine Environment Monitoring Service) data. The code uses a dataset named study_data and performs the following steps:\n\n**Data Preprocessing:**\nRows with missing values are dropped from the study_data dataset using study_data = study_data.dropna().\n\n**Column Initialization:**\nNew columns (VHM0, VMDR, VTPK, Temperature, Salinity, Thickness) are added to the study_data DataFrame, each initialized with the value 0.0.\n\nData Extraction:\nThe extract_model function is defined to extract specific data from other datasets (ds_wav and ds_phy) based on the values in each row of the study_data DataFrame.\nFor each row in study_data:\nThe latitude (lat), longitude (lon), and time (time) values are rounded and converted to strings.\nData is extracted from the ds_wav (wave data) and ds_phy (physical data) datasets using the rounded latitude, longitude, and time values.\nExtracted values are assigned to the corresponding columns in the study_data DataFrame.\nProcessing Progress Bar:\n\nThe tqdm library is used to create a progress bar that shows the processing progress for the iteration through rows in study_data.\nFunction Invocation and Data Display:\n\nThe extract_model function is called to perform the data extraction and assignment for each row.\nRows with missing values are dropped again from the study_data DataFrame.\nThe processed data is displayed.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Create new columns with initial values\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\nstudy_data['Temperature'] = 0.0\nstudy_data['Salinity'] = 0.0\nstudy_data['Thickness'] = 0.0\n\ndef extract_model():\n    # Use tqdm to create a progress bar\n    for index, row in tqdm(study_data.iterrows(), total=len(study_data), desc=\"Processing\"):\n        lat = round(row['LAT'], 1)\n        lon = round(row['LON'], 1)\n        time = str(row['EstimatedTime'])\n\n        VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n        study_data.at[index, 'VHM0'] = VHM0\n        study_data.at[index, 'VMDR'] = VMDR\n        study_data.at[index, 'VTPK'] = VTPK\n        study_data.at[index, 'Temperature'] = thetao\n        study_data.at[index, 'Salinity'] = salin\n        study_data.at[index, 'Thickness'] = thickness\n\n# Call the function to extract the model\nextract_model()\n\n# Drop rows with missing values\nstudy_data = study_data.dropna()\n\n# Save merge data (not fully processed)\nstudy_data.to_csv('data/study_data_np.csv')\n\n# Display the processed data\nstudy_data\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:25.588693Z","iopub.execute_input":"2023-11-22T07:27:25.589069Z","iopub.status.idle":"2023-11-22T07:29:04.497682Z","shell.execute_reply.started":"2023-11-22T07:27:25.589041Z","shell.execute_reply":"2023-11-22T07:29:04.495343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from tqdm import tqdm\nimport pandas as pd\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Create new columns with initial values\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\nstudy_data['Temperature'] = 0.0\nstudy_data['Salinity'] = 0.0\nstudy_data['Thickness'] = 0.0\n\ndef extract_model(chunk):\n    # Use tqdm to create a progress bar\n    for index, row in tqdm(chunk.iterrows(), total=len(chunk), desc=\"Processing\"):\n        lat = round(row['LAT'], 1)\n        lon = round(row['LON'], 1)\n        time = str(row['EstimatedTime'])\n\n        VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n        study_data.at[index, 'VHM0'] = VHM0\n        study_data.at[index, 'VMDR'] = VMDR\n        study_data.at[index, 'VTPK'] = VTPK\n        study_data.at[index, 'Temperature'] = thetao\n        study_data.at[index, 'Salinity'] = salin\n        study_data.at[index, 'Thickness'] = thickness\n\n# Define the chunk size (adjust as needed)\nchunk_size = 100000\n\n# Split the data into chunks\nchunks = [study_data[i:i + chunk_size] for i in range(0, len(study_data), chunk_size)]\n\n# Process each chunk and store the results\nfor chunk in chunks:\n    extract_model(chunk)\n\n# Drop rows with missing values\nstudy_data = study_data.dropna()\n\n# Display the processed data\nstudy_data\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.560064Z","iopub.status.idle":"2023-11-22T07:27:14.560487Z","shell.execute_reply.started":"2023-11-22T07:27:14.560284Z","shell.execute_reply":"2023-11-22T07:27:14.560307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"#to check processing progress\nfrom tqdm import tqdm\n\n\n# Merge AIS and CMEMS datafrom tqdm import tqdm\n\n# Define the chunk size (adjust as needed)\nchunk_size = 100000\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Initialize empty lists to store results\nvhm0_values = []\nvmdr_values = []\nvtpk_values = []\ntemperature_values = []\nsalinity_values = []\nthickness_values = []\n\ndef extract_model(chunk):\n    vhm0_chunk = []\n    vmdr_chunk = []\n    vtpk_chunk = []\n    temperature_chunk = []\n    salinity_chunk = []\n    thickness_chunk = []\n\n    for index, row in tqdm(chunk.iterrows(), total=len(chunk), desc=\"Processing\"):\n        lat = round(row['LAT'], 1)\n        lon = round(row['LON'], 1)\n        time = str(row['EstimatedTime'])\n\n        VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n        vhm0_chunk.append(VHM0)\n        vmdr_chunk.append(VMDR)\n        vtpk_chunk.append(VTPK)\n        temperature_chunk.append(thetao)\n        salinity_chunk.append(salin)\n        thickness_chunk.append(thickness)\n\n    return vhm0_chunk, vmdr_chunk, vtpk_chunk, temperature_chunk, salinity_chunk, thickness_chunk\n\n# Split the data into smaller chunks\nchunks = [study_data[i:i + chunk_size] for i in range(0, len(study_data), chunk_size)]\n\n# Process each chunk and store the results\nfor chunk in chunks:\n    vhm0_chunk, vmdr_chunk, vtpk_chunk, temperature_chunk, salinity_chunk, thickness_chunk = extract_model(chunk)\n    vhm0_values.extend(vhm0_chunk)\n    vmdr_values.extend(vmdr_chunk)\n    vtpk_values.extend(vtpk_chunk)\n    temperature_values.extend(temperature_chunk)\n    salinity_values.extend(salinity_chunk)\n    thickness_values.extend(thickness_chunk)\n\n# Assign the results to the DataFrame\nstudy_data['VHM0'] = vhm0_values\nstudy_data['VMDR'] = vmdr_values\nstudy_data['VTPK'] = vtpk_values\nstudy_data['Temperature'] = temperature_values\nstudy_data['Salinity'] = salinity_values\nstudy_data['Thickness'] = thickness_values\n\n# Now, study_data contains the processed data\nstudy_data = study_data.dropna()\n\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\nstudy_data['Temperature'] = 0.0\nstudy_data['Salinity'] = 0.0\nstudy_data['Thickness'] = 0.0\n\ndef extract_model():\n  \n  for index, row in study_data.iterrows():\n\n      lat = round(row['LAT'], 1)\n      lon = round(row['LON'], 1)\n      time = str(row['EstimatedTime'])\n\n      VHM0 = ds_wav.VHM0.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n      VMDR = ds_wav.VMDR.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n      VTPK = ds_wav.VTPK.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n      thetao = ds_phy.thetao.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')\n      salin = ds_phy.so.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')\n      thickness = ds_phy.mlotst.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n\n      study_data.at[index, 'VHM0'] = VHM0\n      study_data.at[index, 'VMDR'] = VMDR\n      study_data.at[index, 'VTPK'] = VTPK\n      study_data.at[index, 'Temperature'] = thetao\n      study_data.at[index, 'Salinity'] = salin\n      study_data.at[index, 'Thickness'] = thickness\n      \nextract_model()\n\nstudy_data = study_data.dropna()\n\n# Save merge data (not fully processed)\nstudy_data.to_csv('data/study_data_np.csv')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.562485Z","iopub.status.idle":"2023-11-22T07:27:14.563174Z","shell.execute_reply.started":"2023-11-22T07:27:14.562965Z","shell.execute_reply":"2023-11-22T07:27:14.562986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **study_data = study_data.dropna()**: This line removes rows with missing values (NaN) from the study_data DataFrame.\n\n* The next set of lines initializes columns in the study_data DataFrame with zeros. These columns are intended to store various data points from the CMEMS datasets: VHM0, VMDR, VTPK, Temperature, Salinity, and Thickness.\n\n* **def extract_model()**:: This line defines a function called extract_model.\n\n* for index, row in study_data.iterrows():: This line starts a loop that iterates through each row in the study_data DataFrame. For each row, the code will extract data from the CMEMS datasets based on the latitude, longitude, and estimated time for that row.\n\n* **lat = round(row['LAT'], 1)**: This line extracts the latitude value from the current row and rounds it to one decimal place.\n\n* **lon = round(row['LON'], 1)**: This line extracts the longitude value from the current row and rounds it to one decimal place.\n\n* **time = str(row['EstimatedTime'])**: This line extracts the estimated time value from the current row and converts it to a string.\n\n* The following lines use the **.sel()** method to extract data from the CMEMS datasets (e.g., ds_wav.VHM0.sel(...), ds_phy.thetao.sel(...), etc.) based on the latitude, longitude, and time values obtained from the current row.\n\n* The extracted data is then assigned to the corresponding columns in the study_data DataFrame (e.g., study_data.at[index, 'VHM0'] = VHM0). This essentially populates the DataFrame with the CMEMS data for each row in the study_data DataFrame.\n\n* Finally, after processing all rows, the code calls **extract_model()** to perform the data extraction. Then, it removes any rows in study_data that still have missing values (NaN) using **study_data = study_data.dropna()**.\n\nThe purpose of this code is to enrich the study_data DataFrame with CMEMS data based on the provided latitude, longitude, and estimated time. It's essentially merging the AIS data with relevant CMEMS data for further analysis.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize COG\nstudy_data['COG_norm'] = (study_data['COG'] - study_data['COG'].mean(axis = 0)) / study_data['COG'].std(axis = 0)\n\n# Make histogram of normalized COG\nstudy_data['COG_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized COG', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized COG', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['COG_norm'] = ((study_data['COG_norm'] - study_data['COG_norm'].min(axis=0)) / \n                           (study_data['COG_norm'].max(axis = 0) - study_data['COG_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.564521Z","iopub.status.idle":"2023-11-22T07:27:14.565558Z","shell.execute_reply.started":"2023-11-22T07:27:14.565349Z","shell.execute_reply":"2023-11-22T07:27:14.565370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize Gross Tonnage\nstudy_data['GrossTonnage_norm'] = (study_data['GrossTonnage'] - study_data['GrossTonnage'].mean(axis = 0)) / study_data['GrossTonnage'].std(axis = 0)\n\n# Make histogram of normalized Gross Tonnage\nstudy_data['GrossTonnage_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Gross Tonnage', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Gross Tonnage', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['GrossTonnage_norm'] = ((study_data['GrossTonnage_norm'] - study_data['GrossTonnage_norm'].min(axis=0)) / \n                                    (study_data['GrossTonnage_norm'].max(axis = 0) - study_data['GrossTonnage_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.566906Z","iopub.status.idle":"2023-11-22T07:27:14.567625Z","shell.execute_reply.started":"2023-11-22T07:27:14.567419Z","shell.execute_reply":"2023-11-22T07:27:14.567440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize VHM0\nstudy_data['VHM0_norm'] = (study_data['VHM0'] - study_data['VHM0'].mean(axis = 0)) / study_data['VHM0'].std(axis = 0)\n\n# Make histogram of normalized VHM0\nstudy_data['VHM0_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized VHM0', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized VHM0', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Remove VHM0 outliers\nstudy_data = study_data[(study_data['VHM0_norm'] > -2)].dropna()\n\n# Standardize to 0 to 1\nstudy_data['VHM0_norm'] = ((study_data['VHM0_norm'] - study_data['VHM0_norm'].min(axis=0)) / \n                            (study_data['VHM0_norm'].max(axis = 0) - study_data['VHM0_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.568732Z","iopub.status.idle":"2023-11-22T07:27:14.569740Z","shell.execute_reply.started":"2023-11-22T07:27:14.569503Z","shell.execute_reply":"2023-11-22T07:27:14.569524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize VMDR\nstudy_data['VMDR_norm'] = (study_data['VMDR'] - study_data['VMDR'].mean(axis = 0)) / study_data['VMDR'].std(axis = 0)\n\n# Make histogram of normalized VMDR\nstudy_data['VMDR_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized VMDR', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized VMDR', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['VMDR_norm'] = ((study_data['VMDR_norm'] - study_data['VMDR_norm'].min(axis=0)) / \n                            (study_data['VMDR_norm'].max(axis = 0) - study_data['VMDR_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.571108Z","iopub.status.idle":"2023-11-22T07:27:14.571728Z","shell.execute_reply.started":"2023-11-22T07:27:14.571507Z","shell.execute_reply":"2023-11-22T07:27:14.571530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize VTPK\nstudy_data['VTPK_norm'] = (study_data['VTPK'] - study_data['VTPK'].mean(axis = 0)) / study_data['VTPK'].std(axis = 0)\n\n# Make histogram of normalized VTPK\nstudy_data['VTPK_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized VTPK', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized VTPK', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['VTPK_norm'] = ((study_data['VTPK_norm'] - study_data['VTPK_norm'].min(axis=0)) / \n                            (study_data['VTPK_norm'].max(axis=0) - study_data['VTPK_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.573324Z","iopub.status.idle":"2023-11-22T07:27:14.573990Z","shell.execute_reply.started":"2023-11-22T07:27:14.573789Z","shell.execute_reply":"2023-11-22T07:27:14.573809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize Temperature\nstudy_data['Temperature_norm'] = (study_data['Temperature'] - study_data['Temperature'].mean(axis = 0)) / study_data['Temperature'].std(axis = 0)\n\n# Make histogram of normalized Temperature\nstudy_data['Temperature_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Temperature', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Temperature', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['Temperature_norm'] = ((study_data['Temperature_norm'] - study_data['Temperature_norm'].min(axis=0)) / \n                                   (study_data['Temperature_norm'].max(axis = 0) - study_data['Temperature_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.575244Z","iopub.status.idle":"2023-11-22T07:27:14.575675Z","shell.execute_reply.started":"2023-11-22T07:27:14.575482Z","shell.execute_reply":"2023-11-22T07:27:14.575505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize Salinity\nstudy_data['Salinity_norm'] = (study_data['Salinity'] - study_data['Salinity'].mean(axis = 0)) / study_data['Salinity'].std(axis = 0)\n\n# Make histogram of normalized Salinity\nstudy_data['Salinity_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Salinity', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Salinity', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['Salinity_norm'] = ((study_data['Salinity_norm'] - study_data['Salinity_norm'].min(axis=0)) / \n                               (study_data['Salinity_norm'].max(axis=0) - study_data['Salinity_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.577098Z","iopub.status.idle":"2023-11-22T07:27:14.577546Z","shell.execute_reply.started":"2023-11-22T07:27:14.577353Z","shell.execute_reply":"2023-11-22T07:27:14.577372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize Thickness\nstudy_data['Thickness_norm'] = (study_data['Thickness'] - study_data['Thickness'].mean(axis = 0)) / study_data['Thickness'].std(axis = 0)\n\n# Make histogram of normalized Thickness\nstudy_data['Thickness_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Thickness', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Thickness', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['Thickness_norm'] = ((study_data['Thickness_norm'] - study_data['Thickness_norm'].min(axis=0)) / \n                               (study_data['Thickness_norm'].max(axis=0) - study_data['Thickness_norm'].min(axis=0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.579115Z","iopub.status.idle":"2023-11-22T07:27:14.579833Z","shell.execute_reply.started":"2023-11-22T07:27:14.579622Z","shell.execute_reply":"2023-11-22T07:27:14.579644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"After preprocessing...\")\nfinal_data = study_data.filter(['EstimatedTime', 'LAT', 'LON', 'Heading', 'SOG_norm', 'COG_norm', 'GrossTonnage_norm', 'VHM0_norm', 'VMDR_norm', 'VTPK_norm', 'Temperature_norm', 'Salinity_norm', 'Thickness_norm'])\nfinal_data = final_data.dropna()\nfinal_data.to_csv('data/preprocessed_data.csv')\nfinal_data","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:27:14.581123Z","iopub.status.idle":"2023-11-22T07:27:14.581577Z","shell.execute_reply.started":"2023-11-22T07:27:14.581343Z","shell.execute_reply":"2023-11-22T07:27:14.581362Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
